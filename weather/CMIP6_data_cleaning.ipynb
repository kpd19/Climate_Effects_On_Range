{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9fa77067",
   "metadata": {},
   "source": [
    "# Import CMIP6 Climate data, aggregate monthly averages and interpolate to different grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e99323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import flox\n",
    "import cftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a9200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22444fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591c4f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listdir_nohidden(path):\n",
    "    return glob.glob(os.path.join(path, '*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ec3d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_sums(filename, var, var_name):\n",
    "\n",
    "    dataset = xr.open_dataset(filename, decode_times = True)\n",
    "\n",
    "    dataset = dataset.sel(lon = slice(lon_min,lon_max), lat = slice(lat_min,lat_max))\n",
    "\n",
    "    data_interp = dataset.interp(lon = lon_arr, lat = lat_arr)\n",
    "    \n",
    "    # faster to do this on the xarray\n",
    "    data_interp['year'] = data_interp['time'].dt.strftime('%Y')\n",
    "    data_interp['month'] = data_interp['time'].dt.strftime('%B')\n",
    "    data_interp['day'] = data_interp['time'].dt.strftime('%d')\n",
    "    #data_interp['time'] = data_interp['time'].dt.strftime('%r')\n",
    "    \n",
    "    df = data_interp.to_dataframe()\n",
    "    df = df.reset_index()\n",
    "\n",
    "    #df = df[(df['lat'].isin(state_coords.lat_coord2)) & (df['lon'].isin(state_coords.lon_coord2))]\n",
    "\n",
    "    df_means = df.groupby(['lat','lon','month','year']).agg({var:['sum']})\n",
    "    df_means.columns = [var_name]\n",
    "    df_means = df_means.reset_index()\n",
    "\n",
    "    \n",
    "    return(df_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59a43de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_means(filename, var, var_name):\n",
    "\n",
    "    dataset = xr.open_dataset(filename, decode_times = True)\n",
    "\n",
    "    dataset = dataset.sel(lon = slice(lon_min,lon_max), lat = slice(lat_min,lat_max))\n",
    "    data_interp = dataset.interp(lon = lon_arr, lat = lat_arr)\n",
    "\n",
    "    # faster to do this on the xarray\n",
    "    data_interp['year'] = data_interp['time'].dt.strftime('%Y')\n",
    "    data_interp['month'] = data_interp['time'].dt.strftime('%B')\n",
    "    data_interp['day'] = data_interp['time'].dt.strftime('%d')\n",
    "    #data_interp['time'] = data_interp['time'].dt.strftime('%r')\n",
    "    \n",
    "    df = data_interp.to_dataframe()\n",
    "    df = df.reset_index()\n",
    "\n",
    "    #df = df[(df['lat'].isin(state_coords.lat_coord2)) & (df['lon'].isin(state_coords.lon_coord2))]\n",
    "\n",
    "    df_means = df.groupby(['lat','lon','month','year']).agg({var:['mean']})\n",
    "    df_means.columns = [var_name]\n",
    "    df_means = df_means.reset_index()\n",
    "\n",
    "    \n",
    "    return(df_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69141dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gdd(tminf, tmaxf):\n",
    "\n",
    "    dataset_tmin = xr.open_dataset(tminf, decode_times = True)\n",
    "    dataset_tmax = xr.open_dataset(tmaxf, decode_times = True)\n",
    "\n",
    "    dataset_tmin = dataset_tmin.sel(lon = slice(lon_min,lon_max), lat = slice(lat_min,lat_max))\n",
    "    dataset_tmax = dataset_tmax.sel(lon = slice(lon_min,lon_max), lat = slice(lat_min,lat_max))\n",
    "\n",
    "    tmin_interp = dataset_tmin.interp(lon = lon_arr, lat = lat_arr)\n",
    "    tmax_interp = dataset_tmax.interp(lon = lon_arr, lat = lat_arr)\n",
    "\n",
    "    tmin_interp = tmin_interp.merge(tmax_interp)\n",
    "\n",
    "    tmin_interp['year'] = tmin_interp['time'].dt.strftime('%Y')\n",
    "\n",
    "    df = tmin_interp.to_dataframe()\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df['tasmax'] = df['tasmax'] - 273.15\n",
    "    df['tasmin'] = df['tasmin'] - 273.15\n",
    "\n",
    "    df['gdd'] = (df['tasmax'] + df['tasmin'])/2 - 5.6\n",
    "    df['gdd'] = np.where(df['gdd'] < 0, 0, df['gdd'])\n",
    "\n",
    "    df['gdd_sum'] = df.groupby(['lat','lon']).cumsum()['gdd']\n",
    "\n",
    "    #df2 = df[df['gdd_sum'] < 1000]\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2622577c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season_gdd(tminf, tmaxf):\n",
    "\n",
    "    dataset_tmin = xr.open_dataset(tminf, decode_times = True)\n",
    "    dataset_tmax = xr.open_dataset(tmaxf, decode_times = True)\n",
    "\n",
    "    dataset_tmin = dataset_tmin.sel(lon = slice(lon_min,lon_max), lat = slice(lat_min,lat_max))\n",
    "    dataset_tmax = dataset_tmax.sel(lon = slice(lon_min,lon_max), lat = slice(lat_min,lat_max))\n",
    "\n",
    "    tmin_interp = dataset_tmin.interp(lon = lon_arr, lat = lat_arr)\n",
    "    tmax_interp = dataset_tmax.interp(lon = lon_arr, lat = lat_arr)\n",
    "\n",
    "    tmin_interp = tmin_interp.merge(tmax_interp)\n",
    "\n",
    "    tmin_interp['year'] = tmin_interp['time'].dt.strftime('%Y')\n",
    "\n",
    "    #tmin_interp['time'] = tmin_interp.indexes['time'].to_datetimeindex()\n",
    "\n",
    "    df = tmin_interp.to_dataframe()\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df['tasmax'] = df['tasmax'] - 273.15\n",
    "    df['tasmin'] = df['tasmin'] - 273.15\n",
    "\n",
    "    df['gdd'] = (df['tasmax'] + df['tasmin'])/2 - 5.6\n",
    "    df['gdd'] = np.where(df['gdd'] < 0, 0, df['gdd'])\n",
    "\n",
    "    df['gdd_sum'] = df.groupby(['lat','lon']).cumsum()['gdd']\n",
    "\n",
    "    df['julian'] = pd.DatetimeIndex(df['time']).dayofyear\n",
    "    hatch_pred = df[df.gdd_sum>= 300].groupby(['lat','lon','year']).min('julian')\n",
    "    hatch_pred['julian'] = hatch_pred['julian'] + 69\n",
    "\n",
    "    hatch_pred_small = hatch_pred.reset_index()[['lat','lon','year','julian']]\n",
    "\n",
    "    gdd_before = df[df.gdd_sum< 300].groupby(['lat','lon','year']).max('julian').reset_index()[['lat','lon','year','gdd_sum']].rename(columns = {'gdd_sum' : 'gdd_subtract'})\n",
    "\n",
    "    season_gdds = pd.merge(hatch_pred_small,df, how = 'left')\n",
    "\n",
    "    season_gdds = pd.merge(season_gdds,gdd_before,how = 'left')\n",
    "\n",
    "    season_gdds['gdd_season'] = season_gdds['gdd_sum'] - season_gdds['gdd_subtract']\n",
    "    season_gdds = season_gdds[['lat','lon','year','julian','gdd_season']]\n",
    "\n",
    "    return(season_gdds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a4ba9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season_gdd_360(tminf, tmaxf):\n",
    "\n",
    "    dataset_tmin = xr.open_dataset(tminf, decode_times = True)\n",
    "    dataset_tmax = xr.open_dataset(tmaxf, decode_times = True)\n",
    "\n",
    "    dataset_tmin = dataset_tmin.sel(lon = slice(lon_min,lon_max), lat = slice(lat_min,lat_max))\n",
    "    dataset_tmax = dataset_tmax.sel(lon = slice(lon_min,lon_max), lat = slice(lat_min,lat_max))\n",
    "\n",
    "    tmin_interp = dataset_tmin.interp(lon = lon_arr, lat = lat_arr)\n",
    "    tmax_interp = dataset_tmax.interp(lon = lon_arr, lat = lat_arr)\n",
    "\n",
    "    tmin_interp = tmin_interp.merge(tmax_interp)\n",
    "\n",
    "    tmin_interp['year'] = tmin_interp['time'].dt.strftime('%Y')\n",
    "    tmin_interp['month'] = tmin_interp['time'].dt.strftime('%m')\n",
    "    tmin_interp['day'] = tmin_interp['time'].dt.strftime('%d')\n",
    "\n",
    "    df = tmin_interp.to_dataframe()\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df.month =  pd.to_numeric(df.month, errors='coerce')\n",
    "    df.day = pd.to_numeric(df.day, errors='coerce')\n",
    "\n",
    "    df['julian'] = (df.month-1)*30 + df.day\n",
    "\n",
    "    df['tasmax'] = df['tasmax'] - 273.15\n",
    "    df['tasmin'] = df['tasmin'] - 273.15\n",
    "\n",
    "    df['gdd'] = (df['tasmax'] + df['tasmin'])/2 - 5.6\n",
    "    df['gdd'] = np.where(df['gdd'] < 0, 0, df['gdd'])\n",
    "\n",
    "    df['gdd_sum'] = df.groupby(['lat','lon']).cumsum()['gdd']\n",
    "\n",
    "    hatch_pred = df[df.gdd_sum>= 300].groupby(['lat','lon','year']).min('julian')\n",
    "    hatch_pred['julian'] = hatch_pred['julian'] + 69\n",
    "\n",
    "    hatch_pred_small = hatch_pred.reset_index()[['lat','lon','year','julian']]\n",
    "\n",
    "    gdd_before = df[df.gdd_sum< 300].groupby(['lat','lon','year']).max('julian').reset_index()[['lat','lon','year','gdd_sum']].rename(columns = {'gdd_sum' : 'gdd_subtract'})\n",
    "\n",
    "    season_gdds = pd.merge(hatch_pred_small,df, how = 'left')\n",
    "\n",
    "    season_gdds = pd.merge(season_gdds,gdd_before,how = 'left')\n",
    "\n",
    "    season_gdds['gdd_season'] = season_gdds['gdd_sum'] - season_gdds['gdd_subtract']\n",
    "    season_gdds = season_gdds[['lat','lon','year','julian','gdd_season']]\n",
    "\n",
    "    return(season_gdds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8d01fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_min = 32\n",
    "lat_max = 54\n",
    "lon_min = 360 - 128\n",
    "lon_max = 360 - 100\n",
    "\n",
    "lat_arr = np.arange(lat_min,lat_max,0.25)\n",
    "lon_arr = np.arange(lon_min,lon_max,0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1699116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_models = [\"ACCESS-ESM1-5\",\"CanESM5-p1\",\"EC-Earth3-Veg-LR\",\"CNRM-ESM2-f2\",\"GFDL-ESM4\",\"HadGEM3-GC31-MM\",\"INM-CM5-0\",\"KACE-1-0-G\",\"MIROC-ES2L-f2\",\"NorESM2-MM\"]\n",
    "year_sets1 = [\"2030\",\"2050\",\"2070\",\"2090\",\"2041\"]\n",
    "year_sets2 = [\"2040\",\"2060\",\"2080\",\"2100\",\"2089\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1a3b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_id = 2\n",
    "yr_id = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faedd288",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/Volumes/My Book/Climate/CMIP6/\" + climate_models[clim_id] + \"_ssp585_\" + year_sets1[yr_id] + \"/\"\n",
    "mod_name = climate_models[clim_id]\n",
    "year1 = year_sets1[yr_id]\n",
    "year2 = year_sets2[yr_id]\n",
    "\n",
    "print(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717b45b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = listdir_nohidden(model_dir + \"tas/\")\n",
    "\n",
    "tas_df = pd.DataFrame()\n",
    "\n",
    "for file in filenames:\n",
    "\n",
    "    tas_df = tas_df.append(downsample_means(file, 'tas','tas_mean'))\n",
    "\n",
    "tas_df.to_csv(model_dir + \"downsampled/\" + \"means_tas_\" + year1 + \"-\" + year2 + \"_\" + mod_name + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8dface",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = listdir_nohidden(model_dir + \"tasmax/\")\n",
    "\n",
    "#dataset = xr.open_dataset(filenames[1], decode_times = True)\n",
    "\n",
    "tmax_df = pd.DataFrame()\n",
    "\n",
    "for file in filenames:\n",
    "\n",
    "    tmax_df = tmax_df.append(downsample_means(file, 'tasmax','tmax_mean'))\n",
    "\n",
    "tmax_df.to_csv(model_dir + \"downsampled/\" + \"means_tmax_\" + year1 + \"-\" + year2 + \"_\" + mod_name + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009331b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = listdir_nohidden(model_dir + \"tasmin/\")\n",
    "\n",
    "tmin_df = pd.DataFrame()\n",
    "\n",
    "for file in filenames:\n",
    "\n",
    "    tmin_df = tmin_df.append(downsample_means(file, 'tasmin','tmin_mean'))\n",
    "\n",
    "tmin_df.to_csv(model_dir + \"downsampled/\" + \"means_tmin_\" + year1 + \"-\" + year2 + \"_\" + mod_name + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddacca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = listdir_nohidden(model_dir + \"hum/\")\n",
    "\n",
    "hurs_df = pd.DataFrame()\n",
    "\n",
    "for file in filenames:\n",
    "\n",
    "    hurs_df = hurs_df.append(downsample_means(file, 'hurs','hurs_mean'))\n",
    "\n",
    "hurs_df.to_csv(model_dir + \"downsampled/\" + \"means_hurs_\" + year1 + \"-\" + year2 + \"_\" + mod_name + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb6c680",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = listdir_nohidden(model_dir + \"pr/\")\n",
    "\n",
    "pr_df = pd.DataFrame()\n",
    "\n",
    "for file in filenames:\n",
    "\n",
    "    pr_df = pr_df.append(downsample_sums(file, 'pr','sum_pr'))\n",
    "\n",
    "pr_df.to_csv(model_dir + \"downsampled/\" + \"means_pr_\" + year1 + \"-\" +year2 + \"_\" + mod_name + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6138a387",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_tmin = listdir_nohidden(model_dir + \"tasmin/\")\n",
    "file_tmax = listdir_nohidden(model_dir + \"tasmax/\")\n",
    "\n",
    "print(file_tmin[0])\n",
    "print(file_tmax[0])\n",
    "\n",
    "file_len = len(file_tmin[0])\n",
    "\n",
    "if year1 == '2041':\n",
    "    years = np.concatenate((np.arange(2041,2050),np.arange(2061,2070),np.arange(2081,2090)))    \n",
    "else:\n",
    "    years =  np.arange(int(year1),int(year2)+1,1)\n",
    "gdd_df = pd.DataFrame()\n",
    "\n",
    "for yr in years:\n",
    "\n",
    "    tmin_file = file_tmin[0][0:(file_len -7)] + str(yr) + '.nc'\n",
    "    tmax_file = file_tmax[0][0:(file_len -7)] + str(yr) + '.nc'\n",
    "\n",
    "    if mod_name in (\"HadGEM3-GC31-MM\",\"KACE-1-0-G\"):\n",
    "        gdd_df = gdd_df.append(get_season_gdd_360(tmin_file,tmax_file))\n",
    "    else:\n",
    "        gdd_df = gdd_df.append(get_season_gdd(tmin_file,tmax_file))\n",
    "\n",
    "\n",
    "gdd_df.to_csv(model_dir + \"downsampled/\" + \"ALL_season_gdd_\" + year1 + \"-\" + year2 + \"_\" + mod_name + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1105420",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5f4b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_to_df(filename):\n",
    "\n",
    "    dataset = xr.open_dataset(filename, decode_times = True)\n",
    "\n",
    "    dataset = dataset.sel(lon = slice(lon_min,lon_max), lat = slice(lat_min,lat_max))\n",
    "\n",
    "    data_interp = dataset.interp(lon = lon_arr, lat = lat_arr)\n",
    "\n",
    "    # faster to do this on the xarray\n",
    "    data_interp['year'] = data_interp['time'].dt.strftime('%Y')\n",
    "    data_interp['month'] = data_interp['time'].dt.strftime('%B')\n",
    "    data_interp['day'] = data_interp['time'].dt.strftime('%d')\n",
    "    data_interp['time'] = data_interp['time'].dt.strftime('%r')\n",
    "    \n",
    "    df = data_interp.to_dataframe()\n",
    "    df = df.reset_index()\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2059f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'tasmin'\n",
    "\n",
    "filenames = listdir_nohidden(model_dir + var + \"/\")\n",
    "\n",
    "#print(filenames)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for file in filenames:\n",
    "\n",
    "    df = df.append(slice_to_df(file))\n",
    "    print(f\"finished file: {file}\")\n",
    "\n",
    "df.to_csv(model_dir + \"downsampled_int/\" + \"ALL_\" + var + \"_\" + year1 + \"-\" + year2 + \"_GFDL-ESM4.csv\")\n",
    "\n",
    "print(\"Fini\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7f3d861790d663335a24d21a8cd8792fcbfe11b859c172ce0114b7787222c9a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
