{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9fa77067",
   "metadata": {},
   "source": [
    "## Import ERA-5 Temperature and downsample from hourly to daily and monthly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e99323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import flox\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75336124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_df(dataset):\n",
    "    dataset['t2m'] -= 273.15\n",
    "    dataset.t2m.attrs['units'] = 'deg C'\n",
    "    max_daily = dataset.resample(time='D').max(dim='valid_time')\n",
    "    min_daily = dataset.resample(time='D').min(dim='valid_time')\n",
    "    mean_daily = dataset.resample(time='D').mean(dim='valid_time')\n",
    "    \n",
    "    max_daily = max_daily.rename({'t2m':'max_t2m'})\n",
    "    min_daily = min_daily.rename({'t2m':'min_t2m'})\n",
    "    mean_daily = mean_daily.rename({'t2m':'mean_t2m'})\n",
    "    \n",
    "    merged_data = xr.merge([max_daily,min_daily,mean_daily])\n",
    "    \n",
    "    # faster to do this on the xarray\n",
    "    merged_data['year'] = merged_data['time'].dt.strftime('%Y')\n",
    "    merged_data['month'] = merged_data['time'].dt.strftime('%B')\n",
    "    merged_data['day'] = merged_data['time'].dt.strftime('%d')\n",
    "    \n",
    "    df = merged_data.to_dataframe()\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba21dc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listdir_nohidden(path):\n",
    "    return glob.glob(os.path.join(path, '*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718a4727",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/Volumes/My Book/Climate/ERA/gaps/ALL_old2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329c396c",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = \"WUS\"\n",
    "state_code = \"WUS\" # \"AZ_new\"\n",
    "yr1 = 1960\n",
    "yr2 = 1978"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89985ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = xr.open_dataset(model_dir + state_code+\"_t2m_\" + str(yr1) + \"-\" + str(yr2) + \".nc\", decode_times = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d62e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = downsample_df(f1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910efdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df1]) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054b8d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv(\"downsampled/\"+state+\"_t2m_\" + str(yr1) + \"-\" + str(yr2) + \".csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8633a573",
   "metadata": {},
   "source": [
    "### Summary statistics- Converting from daily to monthly mean, maximum, and minimum temperature at 2 meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa26d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_temps(filename):\n",
    "\n",
    "    dataset = xr.open_dataset(filename)\n",
    "\n",
    "    dataset['t2m'] -= 273.15\n",
    "    dataset.t2m.attrs['units'] = 'deg C'\n",
    "    max_daily = dataset.resample(time='D').max(dim='time')\n",
    "    min_daily = dataset.resample(time='D').min(dim='time')\n",
    "    mean_daily = dataset.resample(time='D').mean(dim='time')\n",
    "\n",
    "    max_daily = max_daily.rename({'t2m':'max_t2m'})\n",
    "    min_daily = min_daily.rename({'t2m':'min_t2m'})\n",
    "    mean_daily = mean_daily.rename({'t2m':'mean_t2m'})\n",
    "\n",
    "    merged_data = xr.merge([max_daily,min_daily,mean_daily])\n",
    "\n",
    "    # faster to do this on the xarray\n",
    "    merged_data['year'] = merged_data['time'].dt.strftime('%Y')\n",
    "    merged_data['month'] = merged_data['time'].dt.strftime('%B')\n",
    "    merged_data['day'] = merged_data['time'].dt.strftime('%d')\n",
    "\n",
    "    df = merged_data.to_dataframe()\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df_means = df.groupby(['latitude','longitude','month','year'])[['min_t2m','max_t2m','mean_t2m']].mean()\n",
    "    df_means = df_means.reset_index()\n",
    "\n",
    "    df['gdd'] = (df['max_t2m'] + df['min_t2m'])/2 - 5.6\n",
    "    df['gdd'] = np.where(df['gdd'] < 0, 0, df['gdd'])\n",
    "\n",
    "    df['gdd_sum'] = df.groupby(['latitude','longitude']).cumsum()['gdd']\n",
    "\n",
    "    df['julian'] = pd.DatetimeIndex(df['time']).dayofyear\n",
    "\n",
    "    hatch_pred = df[df.gdd_sum>= 300].groupby(['latitude','longitude','year']).min('julian').rename(columns = {'julian' : 'hatch'})\n",
    "    hatch_pred['julian'] = hatch_pred['hatch'] + 69\n",
    "\n",
    "    hatch_pred_small = hatch_pred.reset_index()[['latitude','longitude','year','julian']]\n",
    "\n",
    "    gdd_before = df[df.gdd_sum< 300].groupby(['latitude','longitude','year']).max('julian').reset_index()[['latitude','longitude','year','gdd_sum','julian']].rename(columns = {'gdd_sum' : 'gdd_subtract', 'julian' : 'hatch'})\n",
    "    gdd_before['hatch'] = gdd_before['hatch'] + 1\n",
    "\n",
    "    season_gdds = pd.merge(hatch_pred_small,df, how = 'left')\n",
    "\n",
    "    season_gdds = pd.merge(season_gdds,gdd_before,how = 'left')\n",
    "\n",
    "    season_gdds['gdd_season'] = season_gdds['gdd_sum'] - season_gdds['gdd_subtract']\n",
    "    season_gdds = season_gdds[['latitude','longitude','year','hatch','gdd_season']]\n",
    "\n",
    "    return(df_means,season_gdds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8f0e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "year1 = '1940'\n",
    "year2 = '1959'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3040fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = listdir_nohidden(model_dir)\n",
    "\n",
    "tas_df = pd.DataFrame()\n",
    "gdd_df = pd.DataFrame()\n",
    "\n",
    "for file in filenames:\n",
    "\n",
    "    op = downsample_temps(file)\n",
    "\n",
    "    tas_df = tas_df.append(op[0])\n",
    "    gdd_df = gdd_df.append(op[1])\n",
    "\n",
    "tas_df.to_csv(\"means_t2m_\" + year1 + \"-\" + year2 + \"_ERA5.csv\")\n",
    "gdd_df.to_csv(\"gdd_season_\" + year1 + \"-\" + year2 + \"_ERA5.csv\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36ebf351",
   "metadata": {},
   "source": [
    "# Individual file analysis for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c2ceb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = xr.open_dataset(\"/Volumes/My Book/Climate/ERA/gaps/ALL_old2/ALL_old2_t2m_1950.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788450b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = dataset.rename({'valid_time' : 'time'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f874db",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['t2m'] -= 273.15\n",
    "dataset.t2m.attrs['units'] = 'deg C'\n",
    "max_daily = dataset.resample(time='D').max(dim='time')\n",
    "min_daily = dataset.resample(time='D').min(dim='time')\n",
    "mean_daily = dataset.resample(time='D').mean(dim='time')\n",
    "\n",
    "max_daily = max_daily.rename({'t2m':'max_t2m'})\n",
    "min_daily = min_daily.rename({'t2m':'min_t2m'})\n",
    "mean_daily = mean_daily.rename({'t2m':'mean_t2m'})\n",
    "\n",
    "merged_data = xr.merge([max_daily,min_daily,mean_daily])\n",
    "\n",
    "# faster to do this on the xarray\n",
    "merged_data['year'] = merged_data['time'].dt.strftime('%Y')\n",
    "merged_data['month'] = merged_data['time'].dt.strftime('%B')\n",
    "merged_data['day'] = merged_data['time'].dt.strftime('%d')\n",
    "\n",
    "df = merged_data.to_dataframe()\n",
    "df = df.reset_index()\n",
    "\n",
    "df_means = df.groupby(['latitude','longitude','month','year'])[['min_t2m','max_t2m','mean_t2m']].mean()\n",
    "df_means = df_means.reset_index()\n",
    "\n",
    "df['gdd'] = (df['max_t2m'] + df['min_t2m'])/2 - 5.6\n",
    "df['gdd'] = np.where(df['gdd'] < 0, 0, df['gdd'])\n",
    "\n",
    "df['gdd_sum'] = df.groupby(['latitude','longitude']).cumsum()['gdd']\n",
    "\n",
    "df['julian'] = pd.DatetimeIndex(df['time']).dayofyear\n",
    "hatch_pred = df[df.gdd_sum>= 300].groupby(['latitude','longitude','year']).min('julian').rename(columns = {'julian' : 'hatch'})\n",
    "hatch_pred['julian'] = hatch_pred['hatch'] + 69\n",
    "\n",
    "hatch_pred_small = hatch_pred.reset_index()[['latitude','longitude','year','julian']]\n",
    "\n",
    "gdd_before = df[df.gdd_sum< 300].groupby(['latitude','longitude','year']).max('julian').reset_index()[['latitude','longitude','year','gdd_sum','julian']].rename(columns = {'gdd_sum' : 'gdd_subtract', 'julian' : 'hatch'})\n",
    "gdd_before['hatch'] = gdd_before['hatch'] + 1\n",
    "\n",
    "season_gdds = pd.merge(hatch_pred_small,df, how = 'left')\n",
    "\n",
    "season_gdds = pd.merge(season_gdds,gdd_before,how = 'left')\n",
    "\n",
    "season_gdds['gdd_season'] = season_gdds['gdd_sum'] - season_gdds['gdd_subtract']\n",
    "#season_gdds = season_gdds[['latitude','longitude','year','hatch','julian','gdd_season']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc23d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_means.to_csv(\"/Volumes/My Book/Climate/WUS/downsampled/means_t2m_2024_ERA5.csv\")\n",
    "season_gdds.to_csv(\"/Volumes/My Book/Climate/WUS/downsampled/gdd_season_t2m_2024_ERA5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2d7aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_gdds = season_gdds[['latitude','longitude','year','hatch','julian','gdd_season']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f1eaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_gdds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6faa0574",
   "metadata": {},
   "source": [
    "## Cold Tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f0b1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# egg mortality- -24 and -28 degrees\n",
    "# larval mortality 0 degrees\n",
    "\n",
    "def cold_tolerance(filename):\n",
    "\n",
    "    dataset = xr.open_dataset(filename)\n",
    "\n",
    "    dataset = dataset.rename({'valid_time' : 'time'})\n",
    "\n",
    "    dataset['t2m'] -= 273.15\n",
    "    dataset.t2m.attrs['units'] = 'deg C'\n",
    "    min_daily = dataset.resample(time='D').min(dim='time')\n",
    "\n",
    "    min_daily = min_daily.rename({'t2m':'min_t2m'})\n",
    "    # faster to do this on the xarray\n",
    "    min_daily['year'] = min_daily['time'].dt.strftime('%Y')\n",
    "    min_daily['month'] = min_daily['time'].dt.strftime('%B')\n",
    "    min_daily['day'] = min_daily['time'].dt.strftime('%d')\n",
    "\n",
    "    df = min_daily.to_dataframe()\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df['thresh1'] = np.where(df['min_t2m'] <= -28, 1, 0)\n",
    "    df['thresh2'] = np.where(df['min_t2m'] <= -24, 1, 0)\n",
    "    df['thresh3'] = np.where(df['min_t2m'] <= 0, 1, 0)    \n",
    "    cold_sums = df.groupby(['latitude','longitude','month','year'])[['thresh1','thresh2','thresh3']].sum()\n",
    "    abs_min = df.groupby(['latitude','longitude','month','year'])[['min_t2m']].min()\n",
    "\n",
    "    cold_sums = cold_sums.reset_index()\n",
    "    abs_min = abs_min.reset_index()\n",
    "\n",
    "    cold_df = pd.merge(cold_sums,abs_min, how = 'left')\n",
    "\n",
    "    return(cold_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e84f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/Volumes/My Book/Climate/ERA/gaps/ALL_recent/\"\n",
    "year1 = '1979'\n",
    "year2 = '2024'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7523e625",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = listdir_nohidden(model_dir)\n",
    "\n",
    "tol_df = pd.DataFrame()\n",
    "\n",
    "for file in filenames:\n",
    "\n",
    "    op = cold_tolerance(file)\n",
    "\n",
    "    tol_df = tol_df.append(op)\n",
    "\n",
    "tol_df.to_csv(model_dir + \"../downsampled/cold_tolerance_\" + year1 + \"-\" + year2 + \"_ERA5.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bce6248",
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6655e641",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7f3d861790d663335a24d21a8cd8792fcbfe11b859c172ce0114b7787222c9a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
